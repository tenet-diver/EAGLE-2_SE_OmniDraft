{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAGLE-2 SE OmniDraft Setup and Execution\n",
    "\n",
    "This notebook automates the complete setup process for the EAGLE-2 SE OmniDraft project:\n",
    "1. Clone the repository\n",
    "2. Install dependencies\n",
    "3. Load models (TinyLlama and Meta-Llama)\n",
    "4. Execute the heterogeneous speculative decoding script\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- CUDA-compatible GPU (recommended)\n",
    "- HuggingFace account for model access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Repository Cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "REPO_URL = \"https://github.com/tenet-diver/EAGLE-2_SE_OmniDraft.git\"  # Update with actual repo URL\n",
    "PROJECT_DIR = \"EAGLE-2_SE_OmniDraft\"\n",
    "SCRIPT_NAME = \"heterogeneous_spd.py\"\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository if it doesn't exist\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    print(f\"Cloning repository from {REPO_URL}...\")\n",
    "    result = subprocess.run([\"git\", \"clone\", REPO_URL], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ Repository cloned successfully\")\n",
    "    else:\n",
    "        print(f\"❌ Error cloning repository: {result.stderr}\")\n",
    "        print(\"Note: Update REPO_URL with the correct repository URL\")\n",
    "else:\n",
    "    print(f\"✅ Repository directory '{PROJECT_DIR}' already exists\")\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(f\"Changed to directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if requirements.txt exists, if not create one based on the script dependencies\n",
    "requirements_content = \"\"\"\n",
    "torch>=2.0.0\n",
    "transformers>=4.30.0\n",
    "numpy>=1.21.0\n",
    "accelerate>=0.20.0\n",
    "sentencepiece>=0.1.99\n",
    "protobuf>=3.20.0\n",
    "\"\"\".strip()\n",
    "\n",
    "if not os.path.exists(\"requirements.txt\"):\n",
    "    print(\"Creating requirements.txt...\")\n",
    "    with open(\"requirements.txt\", \"w\") as f:\n",
    "        f.write(requirements_content)\n",
    "    print(\"✅ requirements.txt created\")\n",
    "else:\n",
    "    print(\"✅ requirements.txt already exists\")\n",
    "\n",
    "# Display requirements\n",
    "with open(\"requirements.txt\", \"r\") as f:\n",
    "    print(\"\\nDependencies to install:\")\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"Installing dependencies...\")\n",
    "result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"], \n",
    "                       capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✅ Dependencies installed successfully\")\n",
    "else:\n",
    "    print(f\"❌ Error installing dependencies: {result.stderr}\")\n",
    "    print(\"Stdout:\", result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Installation and Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations\n",
    "try:\n",
    "    import torch\n",
    "    import transformers\n",
    "    import numpy as np\n",
    "    \n",
    "    print(f\"✅ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"✅ Transformers version: {transformers.__version__}\")\n",
    "    print(f\"✅ NumPy version: {np.__version__}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"✅ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"   Available GPUs: {torch.cuda.device_count()}\")\n",
    "    else:\n",
    "        print(\"⚠️  CUDA not available - will use CPU (slower)\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Pre-load and Verify Models\n",
    "\n",
    "Before running the main script, let's verify that we can load the required models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model loading (similar to what the script does)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Model IDs from the script\n",
    "TINY_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "LARGE_ID = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "print(f\"\\nLoading models...\")\n",
    "print(f\"Tiny model: {TINY_ID}\")\n",
    "print(f\"Large model: {LARGE_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tiny model (should be fast)\n",
    "try:\n",
    "    print(\"Loading TinyLlama tokenizer...\")\n",
    "    tiny_tok = AutoTokenizer.from_pretrained(TINY_ID)\n",
    "    print(\"✅ TinyLlama tokenizer loaded\")\n",
    "    \n",
    "    print(\"Loading TinyLlama model...\")\n",
    "    tiny_lm = AutoModelForCausalLM.from_pretrained(TINY_ID).to(DEVICE).eval()\n",
    "    print(\"✅ TinyLlama model loaded\")\n",
    "    \n",
    "    # Test tokenization\n",
    "    test_text = \"Hello, world!\"\n",
    "    tokens = tiny_tok.encode(test_text)\n",
    "    print(f\"Test tokenization: '{test_text}' -> {tokens}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading TinyLlama: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load large model (may require HuggingFace authentication)\n",
    "try:\n",
    "    print(\"Loading Meta-Llama tokenizer...\")\n",
    "    large_tok = AutoTokenizer.from_pretrained(LARGE_ID)\n",
    "    print(\"✅ Meta-Llama tokenizer loaded\")\n",
    "    \n",
    "    print(\"Loading Meta-Llama model (this may take a while)...\")\n",
    "    large_lm = AutoModelForCausalLM.from_pretrained(LARGE_ID).to(DEVICE).eval()\n",
    "    print(\"✅ Meta-Llama model loaded\")\n",
    "    \n",
    "    # Test tokenization\n",
    "    test_text = \"Hello, world!\"\n",
    "    tokens = large_tok.encode(test_text)\n",
    "    print(f\"Test tokenization: '{test_text}' -> {tokens}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading Meta-Llama: {e}\")\n",
    "    print(\"Note: You may need to:\")\n",
    "    print(\"1. Accept the license agreement on HuggingFace\")\n",
    "    print(\"2. Login with: huggingface-cli login\")\n",
    "    print(\"3. Or use a different model ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Execute the Main Script\n",
    "\n",
    "Now let's run the heterogeneous speculative decoding script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the script exists\n",
    "if os.path.exists(SCRIPT_NAME):\n",
    "    print(f\"✅ Found script: {SCRIPT_NAME}\")\n",
    "    \n",
    "    # Display script size and modification time\n",
    "    script_path = Path(SCRIPT_NAME)\n",
    "    stat = script_path.stat()\n",
    "    print(f\"   Size: {stat.st_size} bytes\")\n",
    "    print(f\"   Modified: {stat.st_mtime}\")\n",
    "else:\n",
    "    print(f\"❌ Script not found: {SCRIPT_NAME}\")\n",
    "    print(\"Available files:\")\n",
    "    for file in os.listdir(\".\"):\n",
    "        print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the script\n",
    "if os.path.exists(SCRIPT_NAME):\n",
    "    print(f\"Executing {SCRIPT_NAME}...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Run the script and capture output\n",
    "    result = subprocess.run([sys.executable, SCRIPT_NAME], \n",
    "                           capture_output=True, text=True, timeout=300)  # 5 minute timeout\n",
    "    \n",
    "    print(\"STDOUT:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(\"\\nSTDERR:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ Script executed successfully\")\n",
    "    else:\n",
    "        print(f\"❌ Script failed with return code: {result.returncode}\")\n",
    "else:\n",
    "    print(f\"❌ Cannot execute: {SCRIPT_NAME} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Alternative - Run Script Interactively\n",
    "\n",
    "If you prefer to run the script interactively within the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Import and run the script functions directly\n",
    "try:\n",
    "    # Import the script as a module\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(\"heterogeneous_spd\", SCRIPT_NAME)\n",
    "    hetero_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(hetero_module)\n",
    "    \n",
    "    print(\"✅ Script imported successfully\")\n",
    "    print(\"Available functions:\")\n",
    "    for attr in dir(hetero_module):\n",
    "        if not attr.startswith('_') and callable(getattr(hetero_module, attr)):\n",
    "            print(f\"  - {attr}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error importing script: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a custom test with the imported functions\n",
    "try:\n",
    "    if 'hetero_module' in locals():\n",
    "        print(\"Running custom test...\")\n",
    "        \n",
    "        # Example: Run the heterogeneous speculative decoding function\n",
    "        test_prompt = \"The future of artificial intelligence is\"\n",
    "        print(f\"Test prompt: '{test_prompt}'\")\n",
    "        \n",
    "        # Call the main function from the script\n",
    "        result = hetero_module.heterogeneous_spec_decode(\n",
    "            prompt=test_prompt,\n",
    "            max_new_tokens=50,\n",
    "            K=32,\n",
    "            alpha=0.15\n",
    "        )\n",
    "        \n",
    "        print(f\"Generated text: {result}\")\n",
    "        print(\"✅ Interactive execution completed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in interactive execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "1. ✅ Cloned the repository (or verified it exists)\n",
    "2. ✅ Installed all required dependencies\n",
    "3. ✅ Verified model loading capabilities\n",
    "4. ✅ Executed the heterogeneous speculative decoding script\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different prompts and parameters\n",
    "- Monitor GPU memory usage during execution\n",
    "- Consider using quantized models for better performance\n",
    "- Add logging and performance metrics\n",
    "\n",
    "### Troubleshooting\n",
    "- If models fail to load, check HuggingFace authentication\n",
    "- For CUDA out of memory errors, try reducing batch sizes or using CPU\n",
    "- Update model IDs if newer versions are available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
